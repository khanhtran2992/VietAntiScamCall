#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script sinh dataset h·ªôi tho·∫°i l·ª´a ƒë·∫£o v√† b√¨nh th∆∞·ªùng
T√≠ch h·ª£p c·∫£ hai lo·∫°i h·ªôi tho·∫°i v√†o m·ªôt dataset c√¢n b·∫±ng
"""

import os
import json
import sys
import argparse
import logging
import subprocess
import time
import random
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

class DatasetGenerator:
    """Class sinh dataset h·ªôi tho·∫°i l·ª´a ƒë·∫£o v√† b√¨nh th∆∞·ªùng"""
    
    def __init__(self, api_key: str, base_url: Optional[str] = None, model: str = "gemini-2.0-flash", save_full: bool = False):
        self.api_key = api_key
        self.base_url = base_url or ""  # Empty string for Gemini
        self.model = model
        self.save_full = save_full
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ƒê∆∞·ªùng d·∫´n c√°c th∆∞ m·ª•c
        self.current_dir = Path(__file__).parent
        self.project_root = self.current_dir.parent
        self.dataset_dir = self.project_root / "text_dataset"
        self.fraud_script = self.project_root / "FraudTeleCallGenerator" / "generate_dialogues.py"
        self.normal_script = self.project_root / "NormalTeleCallGenerator" / "generate_normal_dialogues.py"
        
        # T·∫°o th∆∞ m·ª•c dataset
        self.dataset_dir.mkdir(exist_ok=True)
        # C·∫•u h√¨nh logging v·ªõi encoding an to√†n cho Windows
        log_file = self.dataset_dir / f"generator_log_{self.timestamp}.log"
        
        # T·∫°o formatter v·ªõi encoding UTF-8
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        
        # File handler v·ªõi UTF-8
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        
        # Stream handler v·ªõi UTF-8 (Windows safe)
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        # Configure root logger
        logging.basicConfig(
            level=logging.INFO,
            handlers=[file_handler, stream_handler],
            force=True
        )
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"üöÄ Kh·ªüi t·∫°o Dataset Generator")
        self.logger.info(f"   - API: {base_url or 'Gemini (built-in)'}")
        self.logger.info(f"   - Model: {model}")
        self.logger.info(f"   - Dataset dir: {self.dataset_dir}")
        self.logger.info(f"   - Save full dialogues: {self.save_full}")
    
    def generate_fraud_conversations(self, count: int) -> Dict[str, Any]:
        """Sinh h·ªôi tho·∫°i l·ª´a ƒë·∫£o"""
        self.logger.info(f"üö® Sinh {count} h·ªôi tho·∫°i l·ª´a ƒë·∫£o...")
        
        # T·∫°o th∆∞ m·ª•c output
        fraud_dir = self.dataset_dir / f"fraud_{self.timestamp}"
        fraud_dir.mkdir(exist_ok=True)
        full_dir: Optional[Path] = None
        if self.save_full:
            full_dir = fraud_dir / "full_dialogues"
            full_dir.mkdir(exist_ok=True)
        
        output_file = fraud_dir / "fraud_conversations.jsonl"   
        # Command ƒë·ªÉ ch·∫°y script l·ª´a ƒë·∫£o
        cmd = [
            sys.executable, str(self.fraud_script),
            "--count", str(count),
            "--output", str(output_file),
            "--api_key", self.api_key,
            "--model", self.model,
            "--workers", "1",
            "--max_turns", "25"
        ]
        if self.save_full and full_dir:
            cmd.extend(["--save_full_dialogues", "--full_output_dir", str(full_dir)])
        # Ch·ªâ th√™m base_url n·∫øu ƒë∆∞·ª£c cung c·∫•p (kh√¥ng ph·∫£i Gemini)
        if self.base_url:
            cmd.extend(["--base_url", self.base_url])
        
        try:
            self.logger.info(f"   Ch·∫°y l·ªánh: {' '.join(cmd[:3])} [v·ªõi API params]")
            
            # Set environment for UTF-8 encoding
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            
            # S·ª≠ d·ª•ng Popen ƒë·ªÉ stream output real-time
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                cwd=str(self.fraud_script.parent),
                encoding='utf-8',
                errors='replace',
                env=env,
                bufsize=1,
                universal_newlines=True
            )
              # Stream output real-time
            stdout_lines = []
            if process.stdout:
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        # In ra terminal v√† l∆∞u v√†o log
                        line = output.strip()
                        stdout_lines.append(line)
                        # Th√™m prefix ƒë·ªÉ ph√¢n bi·ªát
                        print(f"[FRAUD] {line}")
            
            return_code = process.poll()
            
            if return_code == 0:
                self.logger.info(f"‚úÖ Sinh h·ªôi tho·∫°i l·ª´a ƒë·∫£o th√†nh c√¥ng")
                result = {
                    "status": "success",
                    "count": count,
                    "output_file": str(output_file),
                }
                if full_dir:
                    result["full_dir"] = str(full_dir)
                return result
            else:
                self.logger.error(f"‚ùå L·ªói sinh h·ªôi tho·∫°i l·ª´a ƒë·∫£o (exit code: {return_code})")
                return {
                    "status": "error", 
                    "error": f"Process exited with code {return_code}",
                    "stdout": stdout_lines
                }
                
        except Exception as e:
            self.logger.error(f"‚ùå Exception sinh h·ªôi tho·∫°i l·ª´a ƒë·∫£o: {e}")
            return {"status": "error", "error": str(e)}
    
    def generate_normal_conversations(self, count: int) -> Dict[str, Any]:
        """Sinh h·ªôi tho·∫°i b√¨nh th∆∞·ªùng"""
        self.logger.info(f"üìû Sinh {count} h·ªôi tho·∫°i b√¨nh th∆∞·ªùng...")
        
        # T·∫°o th∆∞ m·ª•c output
        normal_dir = self.dataset_dir / f"normal_{self.timestamp}"
        normal_dir.mkdir(exist_ok=True)
        full_dir: Optional[Path] = None
        if self.save_full:
            full_dir = normal_dir / "full_dialogues"
            full_dir.mkdir(exist_ok=True)
        
        output_file = normal_dir / "normal_conversations.jsonl"
        # Command ƒë·ªÉ ch·∫°y script b√¨nh th∆∞·ªùng
        cmd = [
            sys.executable, str(self.normal_script),
            "--count", str(count),
            "--output", str(output_file),
            "--api_key", self.api_key,
            "--model", self.model,
            "--workers", "3",
            "--max_turns", "20"
        ]
        if self.save_full and full_dir:
            cmd.extend(["--save_full_dialogues", "--full_output_dir", str(full_dir)])
        # Ch·ªâ th√™m base_url n·∫øu ƒë∆∞·ª£c cung c·∫•p (kh√¥ng ph·∫£i Gemini)
        if self.base_url:
            cmd.extend(["--base_url", self.base_url])
        
        try:
            self.logger.info(f"   Ch·∫°y l·ªánh: {' '.join(cmd[:3])} [v·ªõi API params]")
            
            # Set environment for UTF-8 encoding
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            
            # S·ª≠ d·ª•ng Popen ƒë·ªÉ stream output real-time
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                cwd=str(self.normal_script.parent),
                encoding='utf-8',
                errors='replace',
                env=env,
                bufsize=1,
                universal_newlines=True
            )
            
            # Stream output real-time
            stdout_lines = []
            if process.stdout:
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        # In ra terminal v√† l∆∞u v√†o log
                        line = output.strip()
                        stdout_lines.append(line)
                        # Th√™m prefix ƒë·ªÉ ph√¢n bi·ªát
                        print(f"[NORMAL] {line}")
            
            return_code = process.poll()
            
            if return_code == 0:
                self.logger.info(f"‚úÖ Sinh h·ªôi tho·∫°i b√¨nh th∆∞·ªùng th√†nh c√¥ng")
                result = {
                    "status": "success",
                    "count": count,
                    "output_file": str(output_file),
                }
                if full_dir:
                    result["full_dir"] = str(full_dir)
                return result
            else:
                self.logger.error(f"‚ùå L·ªói sinh h·ªôi tho·∫°i b√¨nh th∆∞·ªùng (exit code: {return_code})")
                return {
                    "status": "error", 
                    "error": f"Process exited with code {return_code}",
                    "stdout": stdout_lines                }
                
        except Exception as e:
            self.logger.error(f"‚ùå Exception sinh h·ªôi tho·∫°i b√¨nh th∆∞·ªùng: {e}")
            return {"status": "error", "error": str(e)}
    
    def merge_and_balance_dataset(self, fraud_file: str, normal_file: str, 
                                 output_name: Optional[str] = None) -> Dict[str, Any]:
        """G·ªôp v√† c√¢n b·∫±ng dataset"""
        if output_name is None:
            output_name = f"balanced_dataset_{self.timestamp}"
        
        output_dir = self.dataset_dir / output_name
        output_dir.mkdir(exist_ok=True)
        
        merged_file = output_dir / "merged_conversations.jsonl"
        
        self.logger.info(f"üîÑ G·ªôp v√† c√¢n b·∫±ng dataset...")
        
        conversations = []
        
        # ƒê·ªçc h·ªôi tho·∫°i l·ª´a ƒë·∫£o
        fraud_count = 0
        if os.path.exists(fraud_file):
            with open(fraud_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        data['label'] = 'fraud'
                        data['is_fraud'] = 1
                        conversations.append(data)
                        fraud_count += 1
        
        # ƒê·ªçc h·ªôi tho·∫°i b√¨nh th∆∞·ªùng
        normal_count = 0
        if os.path.exists(normal_file):
            with open(normal_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        data['label'] = 'normal'
                        data['is_fraud'] = 0
                        conversations.append(data)
                        normal_count += 1
        
        # Tr·ªôn ng·∫´u nhi√™n
        random.shuffle(conversations)
        
        # Ghi file g·ªôp
        with open(merged_file, 'w', encoding='utf-8') as f:
            for conv in conversations:
                f.write(json.dumps(conv, ensure_ascii=False) + '\n')
        
        # T·∫°o th·ªëng k√™
        stats = self.create_dataset_statistics(conversations, output_dir)
        
        result = {
            "status": "success",
            "total_conversations": len(conversations),
            "fraud_count": fraud_count,
            "normal_count": normal_count,
            "fraud_ratio": fraud_count / len(conversations) if conversations else 0,
            "output_file": str(merged_file),
            "output_dir": str(output_dir),
            "stats_file": str(output_dir / "statistics.json")
        }
        
        self.logger.info(f"‚úÖ G·ªôp dataset th√†nh c√¥ng:")
        self.logger.info(f"   - T·ªïng: {len(conversations)} h·ªôi tho·∫°i")
        self.logger.info(f"   - L·ª´a ƒë·∫£o: {fraud_count} ({fraud_count/len(conversations)*100:.1f}%)")
        self.logger.info(f"   - B√¨nh th∆∞·ªùng: {normal_count} ({normal_count/len(conversations)*100:.1f}%)")
        self.logger.info(f"   - File: {merged_file}")
        
        return result
    
    def create_dataset_statistics(self, conversations: List[Dict], output_dir: Path) -> Dict[str, Any]:
        """T·∫°o th·ªëng k√™ chi ti·∫øt dataset"""
        stats = {
            "generation_info": {
                "timestamp": self.timestamp,
                "total_conversations": len(conversations),
                "model": self.model,
                "api_base": self.base_url
            },
            "label_distribution": {"fraud": 0, "normal": 0},
            "fraud_types": {},
            "conversation_types": {},
            "user_demographics": {
                "age_groups": {},
                "occupations": {},
                "awareness_levels": {}
            },
            "conversation_quality": {
                "avg_turns": 0,
                "turn_distribution": {},
                "avg_length": 0
            }
        }
        
        total_turns = 0
        total_length = 0
        
        for conv in conversations:
            # Ph√¢n lo·∫°i c∆° b·∫£n
            label = conv.get('label', 'unknown')
            stats["label_distribution"][label] = stats["label_distribution"].get(label, 0) + 1
            
            # Lo·∫°i l·ª´a ƒë·∫£o
            if label == 'fraud':
                fraud_type = conv.get('fraud_type', 'unknown')
                stats["fraud_types"][fraud_type] = stats["fraud_types"].get(fraud_type, 0) + 1
            
            # Lo·∫°i h·ªôi tho·∫°i b√¨nh th∆∞·ªùng
            if label == 'normal':
                conv_type = conv.get('conversation_type', 'unknown')
                stats["conversation_types"][conv_type] = stats["conversation_types"].get(conv_type, 0) + 1
            
            # Th√¥ng tin ng∆∞·ªùi d√πng
            age = str(conv.get('user_age', 'unknown'))
            occupation = conv.get('occupation', 'unknown')
            awareness = conv.get('user_awareness', 'unknown')
            
            stats["user_demographics"]["age_groups"][age] = stats["user_demographics"]["age_groups"].get(age, 0) + 1
            stats["user_demographics"]["occupations"][occupation] = stats["user_demographics"]["occupations"].get(occupation, 0) + 1
            stats["user_demographics"]["awareness_levels"][awareness] = stats["user_demographics"]["awareness_levels"].get(awareness, 0) + 1
            
            # Ch·∫•t l∆∞·ª£ng h·ªôi tho·∫°i
            dialogue = conv.get('dialogue', [])
            turns = len(dialogue)
            total_turns += turns
            
            turn_range = f"{(turns//5)*5}-{(turns//5)*5+4}"
            stats["conversation_quality"]["turn_distribution"][turn_range] = stats["conversation_quality"]["turn_distribution"].get(turn_range, 0) + 1
            
            # ƒê·ªô d√†i text
            text_length = sum(len(turn.get('content', '')) for turn in dialogue)
            total_length += text_length
        
        # T√≠nh trung b√¨nh
        if conversations:
            stats["conversation_quality"]["avg_turns"] = total_turns / len(conversations)
            stats["conversation_quality"]["avg_length"] = total_length / len(conversations)
        
        # Ghi file th·ªëng k√™
        stats_file = output_dir / "statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        return stats
    
    def generate_balanced_dataset(self, total_count: int, fraud_ratio: float = 0.5) -> Dict[str, Any]:
        """Sinh dataset c√¢n b·∫±ng v·ªõi t·ª∑ l·ªá l·ª´a ƒë·∫£o/b√¨nh th∆∞·ªùng t√πy ch·ªânh"""
        fraud_count = int(total_count * fraud_ratio)
        normal_count = total_count - fraud_count
        
        self.logger.info(f"üéØ Sinh dataset c√¢n b·∫±ng:")
        self.logger.info(f"   - T·ªïng: {total_count} h·ªôi tho·∫°i")
        self.logger.info(f"   - L·ª´a ƒë·∫£o: {fraud_count} ({fraud_ratio*100:.1f}%)")
        self.logger.info(f"   - B√¨nh th∆∞·ªùng: {normal_count} ({(1-fraud_ratio)*100:.1f}%)")
        
        results = {}
        
        # Sinh h·ªôi tho·∫°i l·ª´a ƒë·∫£o
        if fraud_count > 0:
            fraud_result = self.generate_fraud_conversations(fraud_count)
            results['fraud'] = fraud_result
        
        # Sinh h·ªôi tho·∫°i b√¨nh th∆∞·ªùng
        if normal_count > 0:
            normal_result = self.generate_normal_conversations(normal_count)
            results['normal'] = normal_result
        
        # G·ªôp dataset n·∫øu c·∫£ hai th√†nh c√¥ng
        if (results.get('fraud', {}).get('status') == 'success' and 
            results.get('normal', {}).get('status') == 'success'):
            
            merge_result = self.merge_and_balance_dataset(
                results['fraud']['output_file'],
                results['normal']['output_file']
            )
            results['merged'] = merge_result
        
        return results

def main():
    parser = argparse.ArgumentParser(
        description="Sinh dataset h·ªôi tho·∫°i l·ª´a ƒë·∫£o v√† b√¨nh th∆∞·ªùng",
        formatter_class=argparse.RawDescriptionHelpFormatter,        
        epilog="""
        V√≠ d·ª• s·ª≠ d·ª•ng:
        # Sinh 1000 h·ªôi tho·∫°i c√¢n b·∫±ng (50% l·ª´a ƒë·∫£o, 50% b√¨nh th∆∞·ªùng)
        python dataset_generator.py --total 1000 --api_key YOUR_GEMINI_KEY --model gemini-2.0-flash

        # Sinh 500 h·ªôi tho·∫°i v·ªõi 70% l·ª´a ƒë·∫£o
        python dataset_generator.py --total 500 --fraud_ratio 0.7 --api_key YOUR_GEMINI_KEY --model gemini-2.0-flash

        # Ch·ªâ sinh h·ªôi tho·∫°i l·ª´a ƒë·∫£o
        python dataset_generator.py --fraud_only 300 --api_key YOUR_GEMINI_KEY --model gemini-2.0-flash

        # Ch·ªâ sinh h·ªôi tho·∫°i b√¨nh th∆∞·ªùng
        python dataset_generator.py --normal_only 200 --api_key YOUR_GEMINI_KEY --model gemini-2.0-flash
        """
    )
    
    # Nh√≥m tham s·ªë ch√≠nh
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--total", type=int, help="T·ªïng s·ªë h·ªôi tho·∫°i c·∫ßn sinh (s·∫Ω c√¢n b·∫±ng theo fraud_ratio)")
    group.add_argument("--fraud_only", type=int, help="Ch·ªâ sinh h·ªôi tho·∫°i l·ª´a ƒë·∫£o")
    group.add_argument("--normal_only", type=int, help="Ch·ªâ sinh h·ªôi tho·∫°i b√¨nh th∆∞·ªùng")
      # Tham s·ªë API
    parser.add_argument("--api_key", required=True, help="API key (b·∫Øt bu·ªôc)")
    parser.add_argument("--base_url", help="Base URL API (kh√¥ng c·∫ßn cho Gemini)")
    parser.add_argument("--model", default="gemini-2.0-flash", help="Model AI")
    parser.add_argument("--save_full", action="store_true", help="Luu hoi thoai day du (debug)")
    # Tham s·ªë t√πy ch·ªânh
    parser.add_argument("--fraud_ratio", type=float, default=0.5, 
                       help="Ty le hoi thoai lua dao (0.0-1.0, mac dinh 0.5)")
    
    args = parser.parse_args()
    
    # Ki·ªÉm tra tham s·ªë
    if args.fraud_ratio < 0 or args.fraud_ratio > 1:
        parser.error("fraud_ratio ph·∫£i trong kho·∫£ng 0.0-1.0")
      # T·∫°o generator
    generator = DatasetGenerator(args.api_key, args.base_url, args.model, args.save_full)
    
    start_time = time.time()
    results = {}
    
    try:
        if args.total:
            # Sinh dataset c√¢n b·∫±ng
            results = generator.generate_balanced_dataset(args.total, args.fraud_ratio)
            
        elif args.fraud_only:
            # Ch·ªâ sinh l·ª´a ƒë·∫£o
            results = {"fraud": generator.generate_fraud_conversations(args.fraud_only)}
            
        elif args.normal_only:
            # Ch·ªâ sinh b√¨nh th∆∞·ªùng
            results = {"normal": generator.generate_normal_conversations(args.normal_only)}
        
        elapsed = time.time() - start_time
        generator.logger.info(f"üèÅ Ho√†n th√†nh trong {elapsed:.2f} gi√¢y")
        
        # In k·∫øt qu·∫£ t√≥m t·∫Øt
        generator.logger.info("üìã K·∫æT QU·∫¢ T·ªîNG QUAN:")
        for key, result in results.items():
            if result.get('status') == 'success':
                generator.logger.info(f"   ‚úÖ {key}: {result.get('count', '?')} h·ªôi tho·∫°i")
            else:
                generator.logger.info(f"   ‚ùå {key}: {result.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')}")
        
        # Ghi k·∫øt qu·∫£ chi ti·∫øt
        result_file = generator.dataset_dir / f"generation_results_{generator.timestamp}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        generator.logger.info(f"üìÑ Chi ti·∫øt k·∫øt qu·∫£: {result_file}")
        
    except KeyboardInterrupt:
        generator.logger.info("‚ùå B·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng")
        sys.exit(1)
    except Exception as e:
        generator.logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
